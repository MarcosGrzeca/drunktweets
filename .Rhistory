library(text2vec)
library(data.table)
library(SnowballC)
library(keras)
source(file_path_as_absolute("utils/getDados.R"))
source(file_path_as_absolute("baseline/dados.R"))
source(file_path_as_absolute("utils/tokenizer.R"))
dadosTreinarEmbeddings <- getDadosWordEmbeddings()
wiki <- dadosTreinarEmbeddings$textEmbedding
tokens <- space_tokenizer(wiki)
it = itoken(tokens)
stop_words <- tm::stopwords("en")
vocab <- create_vocabulary(it, stopwords = stop_words)
#vectorizer = vocab_vectorizer(vocab, grow_dtm = F, skip_grams_window = 5)
vectorizer <- vocab_vectorizer(vocab)
tcm <- create_tcm(it, vectorizer, skip_grams_window = 5L)
glove <- GlobalVectors$new(word_vectors_size = 100, vocabulary = vocab, x_max = 10)
wv_main <- fit_transform(tcm, glove, n_iter = 30)
wv_context <- glove$components
word_vectors <- wv_main + t(wv_context)
save(word_vectors, file = "ipm/embeddings/skipgram_glove_without_stopwords.Rda")
library(Rtsne)
library(ggplot2)
library(plotly)
#tsne <- Rtsne(word_vectors, perplexity = 50, pca = TRUE)
tsne <- Rtsne(word_vectors[1:1500,], perplexity = 100, pca = TRUE)
tsne_plot <- tsne$Y %>%
as.data.frame() %>%
ggplot(aes(x = V1, y = V2, label = word)) +
geom_text(size = 3)
library(tools)
library(text2vec)
library(data.table)
library(SnowballC)
library(keras)
source(file_path_as_absolute("utils/getDados.R"))
source(file_path_as_absolute("baseline/dados.R"))
source(file_path_as_absolute("utils/tokenizer.R"))
dadosTreinarEmbeddings <- getDadosWordEmbeddings()
wiki <- dadosTreinarEmbeddings$textEmbedding
# create vocabulary
tokens <- space_tokenizer(wiki)
it = itoken(tokens)
stop_words <- tm::stopwords("en")
#vocab <- create_vocabulary(it, stopwords = stop_words)
vocab <- create_vocabulary(it)
#vectorizer = vocab_vectorizer(vocab, grow_dtm = F, skip_grams_window = 5)
vectorizer <- vocab_vectorizer(vocab)
tcm <- create_tcm(it, vectorizer, skip_grams_window = 5L)
glove <- GlobalVectors$new(word_vectors_size = 100, vocabulary = vocab, x_max = 10)
wv_main <- fit_transform(tcm, glove, n_iter = 25)
wv_context <- glove$components
word_vectors <- wv_main + t(wv_context)
#save(word_vectors, file = "ipm/embeddings/skipgram_glove_without_stopwords.Rda")
save(word_vectors, file = "ipm/embeddings/skipgram_glove.Rda")
library(tools)
library(caret)
library(mlbench)
load("ensemble/resultados/exp6/imageFile.RData")
importance <- varImp(treegram25NotNull, scale=FALSE)
# summarize importance
marcos <- print(importance, top = 50)
options(max.print = 99999999)
library(tools)
library(rowr)
library(text2vec)
library(data.table)
library(SnowballC)
source(file_path_as_absolute("utils/getDados.R"))
source(file_path_as_absolute("utils/getDadosAmazon.R"))
source(file_path_as_absolute("utils/tokenizer.R"))
#Configuracoes
DATABASE <- "icwsm"
dados <- getDadosAmazon()
dados <- discretizarTurno(dados)
clearConsole()
setDT(dados)
setkey(dados, id)
it_train = itoken(dados$textParser,
preprocessor = tolower,
tokenizer = word_tokenizer,
ids = dados$id,
progressbar = TRUE)
vocab = create_vocabulary(it_train, stopwords = tm::stopwords("en"), ngram = c(1L, 2L))
vocab = prune_vocabulary(vocab, term_count_min = 2)
dtm_train_texto = create_dtm(it_train, vectorizer)
vectorizer = vocab_vectorizer(vocab)
it_train_hash = itoken(dados$hashtags,
preprocessor = tolower,
tokenizer = word_tokenizer,
ids = dados$id,
progressbar = TRUE)
vocabHashTags = create_vocabulary(it_train_hash)
vocabHashTags = prune_vocabulary(vocabHashTags, term_count_min = 2)
vectorizerHashTags = vocab_vectorizer(vocabHashTags)
dtm_train_hash_tags = create_dtm(it_train_hash, vectorizerHashTags)
it_train = itoken(strsplit(dados$entidades, ","),
preprocessor = tolower,
tokenizer = word_tokenizer,
ids = dados$id,
progressbar = TRUE)
vocab = create_vocabulary(it_train)
vocab = prune_vocabulary(vocab, term_count_min = 2)
vectorizer = vocab_vectorizer(vocab)
dataFrameEntidades = create_dtm(it_train, vectorizer)
it_train = itoken(strsplit(dados$types, ","),
preprocessor = tolower,
tokenizer = word_tokenizer,
ids = dados$id,
progressbar = TRUE)
vocab = create_vocabulary(it_train)
vocab = prune_vocabulary(vocab, term_count_min = 2)
vectorizer = vocab_vectorizer(vocab)
dataFrameTypes = create_dtm(it_train, vectorizer)
#Concatenar resultados
dataFrameTexto <- as.data.frame(as.matrix(dtm_train_texto))
dataFrameHash <- as.data.frame(as.matrix(dtm_train_hash_tags))
dataFrameEntidades <- as.data.frame(as.matrix(dataFrameEntidades))
dataFrameTypes <- as.data.frame(as.matrix(dataFrameTypes))
options(max.print = 99999999)
library(tools)
library(rowr)
library(text2vec)
library(data.table)
library(SnowballC)
source(file_path_as_absolute("utils/getDados.R"))
source(file_path_as_absolute("utils/getDadosAmazon.R"))
source(file_path_as_absolute("utils/tokenizer.R"))
#Configuracoes
DATABASE <- "icwsm"
dados <- getDadosAmazon()
dados <- discretizarTurno(dados)
clearConsole()
setDT(dados)
setkey(dados, id)
it_train = itoken(dados$textParser,
preprocessor = tolower,
tokenizer = word_tokenizer,
ids = dados$id,
progressbar = TRUE)
vocab = create_vocabulary(it_train, stopwords = tm::stopwords("en"), ngram = c(1L, 2L))
vocab = prune_vocabulary(vocab, term_count_min = 2)
vectorizer = vocab_vectorizer(vocab)
dtm_train_texto = create_dtm(it_train, vectorizer)
it_train_hash = itoken(dados$hashtags,
preprocessor = tolower,
tokenizer = word_tokenizer,
ids = dados$id,
progressbar = TRUE)
vocabHashTags = create_vocabulary(it_train_hash)
vocabHashTags = prune_vocabulary(vocabHashTags, term_count_min = 2)
vectorizerHashTags = vocab_vectorizer(vocabHashTags)
dtm_train_hash_tags = create_dtm(it_train_hash, vectorizerHashTags)
it_train = itoken(strsplit(dados$entidades, ","),
preprocessor = tolower,
tokenizer = word_tokenizer,
ids = dados$id,
progressbar = TRUE)
vocab = create_vocabulary(it_train)
vocab = prune_vocabulary(vocab, term_count_min = 2)
vectorizer = vocab_vectorizer(vocab)
dataFrameEntidades = create_dtm(it_train, vectorizer)
it_train = itoken(strsplit(dados$types, ","),
preprocessor = tolower,
tokenizer = word_tokenizer,
ids = dados$id,
progressbar = TRUE)
vocab = create_vocabulary(it_train)
vocab = prune_vocabulary(vocab, term_count_min = 2)
vectorizer = vocab_vectorizer(vocab)
dataFrameTypes = create_dtm(it_train, vectorizer)
#Concatenar resultados
dataFrameTexto <- as.data.frame(as.matrix(dtm_train_texto))
dataFrameHash <- as.data.frame(as.matrix(dtm_train_hash_tags))
dataFrameEntidades <- as.data.frame(as.matrix(dataFrameEntidades))
dataFrameTypes <- as.data.frame(as.matrix(dataFrameTypes))
maFinal <- cbind.fill(subset(dados, select = -c(textParser, id, hashtags, entidades, types, textEmbedding)), dataFrameTexto)
maFinal <- cbind.fill(maFinal, dataFrameHash)
maFinal <- cbind.fill(maFinal, dataFrameEntidades)
maFinal <- cbind.fill(maFinal, dataFrameTypes)
save(maFinal, file = "amazon/rdas/2gram-entidades-erro-pruning.Rda")
options(max.print = 99999999)
library(tools)
library(rowr)
library(text2vec)
library(data.table)
library(SnowballC)
source(file_path_as_absolute("utils/getDados.R"))
source(file_path_as_absolute("utils/getDadosAmazon.R"))
source(file_path_as_absolute("utils/tokenizer.R"))
#Configuracoes
DATABASE <- "icwsm"
dados <- getDadosAmazon()
dados <- discretizarTurno(dados)
clearConsole()
setDT(dados)
setkey(dados, id)
it_train = itoken(dados$textParser,
preprocessor = tolower,
tokenizer = word_tokenizer,
ids = dados$id,
progressbar = TRUE)
vocab = create_vocabulary(it_train, stopwords = tm::stopwords("en"), ngram = c(1L, 2L))
vocab = prune_vocabulary(vocab, term_count_min = 2)
vectorizer = vocab_vectorizer(vocab)
dtm_train_texto = create_dtm(it_train, vectorizer)
it_train_hash = itoken(dados$hashtags,
preprocessor = tolower,
tokenizer = word_tokenizer,
ids = dados$id,
progressbar = TRUE)
vocabHashTags = create_vocabulary(it_train_hash)
vocabHashTags = prune_vocabulary(vocabHashTags, term_count_min = 2)
vectorizerHashTags = vocab_vectorizer(vocabHashTags)
dtm_train_hash_tags = create_dtm(it_train_hash, vectorizerHashTags)
it_train = itoken(strsplit(dados$entidades, ","),
preprocessor = tolower,
tokenizer = word_tokenizer,
ids = dados$id,
progressbar = TRUE)
vocab = create_vocabulary(it_train)
vocab = prune_vocabulary(vocab, term_count_min = 2)
vectorizer = vocab_vectorizer(vocab)
dataFrameEntidades = create_dtm(it_train, vectorizer)
it_train = itoken(strsplit(dados$types, ","),
preprocessor = tolower,
tokenizer = word_tokenizer,
ids = dados$id,
progressbar = TRUE)
vocab = create_vocabulary(it_train)
vocab = prune_vocabulary(vocab, term_count_min = 2)
vectorizer = vocab_vectorizer(vocab)
dataFrameTypes = create_dtm(it_train, vectorizer)
#Concatenar resultados
dataFrameTexto <- as.data.frame(as.matrix(dtm_train_texto))
dataFrameHash <- as.data.frame(as.matrix(dtm_train_hash_tags))
dataFrameEntidades <- as.data.frame(as.matrix(dataFrameEntidades))
dataFrameTypes <- as.data.frame(as.matrix(dataFrameTypes))
maFinal <- cbind.fill(subset(dados, select = -c(textParser, id, hashtags, entidades, types, textEmbedding, textOriginal)), dataFrameTexto)
maFinal <- cbind.fill(maFinal, dataFrameHash)
maFinal <- cbind.fill(maFinal, dataFrameEntidades)
maFinal <- cbind.fill(maFinal, dataFrameTypes)
save(maFinal, file = "amazon/rdas/2gram-entidades-erro-pruning.Rda")
ncol(maFinal)
load("ensemble/resultados/exp5/imageFile_semkeywords.Rda")
load("ensemble/resultados/exp5/imageFile_semkeywords.RData")
resultados
load("ensemble/resultados/exp5/imageFileBaseline.RData")
resultados
library(tools)
fileName <- "ipm/results_q6.Rdata"
source(file_path_as_absolute("ipm/loads.R"))
DESC <- "Exp6 - CNN + Semantic Enrichment + Word embeddings"
for (year in 1:10){
try({
load("amazon/rdas/sequencesexp6.RData")
FLAGS <- flags(
flag_integer("epochs", 4),
flag_integer("batch_size", 64)
)
# Data Preparation --------------------------------------------------------
# Parameters --------------------------------------------------------------
embedding_dims <- 100
filters <- 200
kernel_size <- 10
hidden_dims <- 200
main_input <- layer_input(shape = c(maxlen), dtype = "int32")
ccn_out <- main_input %>%
layer_embedding(vocab_size, embedding_dims, input_length = maxlen) %>%
layer_dropout(0.1) %>%
layer_conv_1d(
filters, kernel_size,
padding = "valid", activation = "relu", strides = 1
) %>%
layer_global_max_pooling_1d() %>%
layer_dense(hidden_dims)
auxiliary_input <- layer_input(shape = c(max_sequence))
entities_out <- auxiliary_input
auxiliary_input_types <- layer_input(shape = c(max_sequence_types))
types_out <- auxiliary_input_types
main_output <- layer_concatenate(c(ccn_out, entities_out, types_out)) %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dropout(0.2) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1, activation = 'sigmoid')
model <- keras_model(
inputs = c(main_input, auxiliary_input, auxiliary_input_types),
outputs = main_output
)
# Compile model
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
history <- model %>%
fit(
x = list(train_vec$new_textParser, sequences, sequences_types),
y = array(dados_train$resposta),
batch_size = FLAGS$batch_size,
epochs = FLAGS$epochs,
validation_split = 0.2
)
predictions <- model %>% predict(list(test_vec$new_textParser, sequences_test, sequences_test_types))
predictions2 <- round(predictions, 0)
matriz <- confusionMatrix(data = as.factor(predictions2), as.factor(dados_test$resposta), positive="1")
resultados <- addRowAdpater(resultados, DESC, matriz)
})
}
resultados$F1
resultados$Precision
resultados$Recall
library(tools)
fileName <- "ipm/results_q6.Rdata"
source(file_path_as_absolute("ipm/loads.R"))
DESC <- "Exp6 - CNN + Semantic Enrichment + Word embeddings"
for (year in 1:5){
try({
load("amazon/rdas/sequencesexp6.RData")
FLAGS <- flags(
flag_integer("epochs", 2),
flag_integer("batch_size", 128)
)
# Data Preparation --------------------------------------------------------
# Parameters --------------------------------------------------------------
embedding_dims <- 100
filters <- 200
kernel_size <- 10
hidden_dims <- 200
main_input <- layer_input(shape = c(maxlen), dtype = "int32")
ccn_out <- main_input %>%
layer_embedding(vocab_size, embedding_dims, input_length = maxlen) %>%
layer_dropout(0.1) %>%
layer_conv_1d(
filters, kernel_size,
padding = "valid", activation = "relu", strides = 1
) %>%
layer_global_max_pooling_1d() %>%
layer_dense(hidden_dims)
auxiliary_input <- layer_input(shape = c(max_sequence))
entities_out <- auxiliary_input
auxiliary_input_types <- layer_input(shape = c(max_sequence_types))
types_out <- auxiliary_input_types
main_output <- layer_concatenate(c(ccn_out, entities_out, types_out)) %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dropout(0.2) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1, activation = 'sigmoid')
model <- keras_model(
inputs = c(main_input, auxiliary_input, auxiliary_input_types),
outputs = main_output
)
# Compile model
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
history <- model %>%
fit(
x = list(train_vec$new_textParser, sequences, sequences_types),
y = array(dados_train$resposta),
batch_size = FLAGS$batch_size,
epochs = FLAGS$epochs,
validation_split = 0.2
)
history
predictions <- model %>% predict(list(test_vec$new_textParser, sequences_test, sequences_test_types))
predictions2 <- round(predictions, 0)
matriz <- confusionMatrix(data = as.factor(predictions2), as.factor(dados_test$resposta), positive="1")
resultados <- addRowAdpater(resultados, DESC, matriz)
})
}
resultados$F1
resultados$Precision
resultados$Recall
library(tools)
fileName <- "ipm/results_q6.Rdata"
source(file_path_as_absolute("ipm/loads.R"))
DESC <- "Exp6 - CNN + Semantic Enrichment + Word embeddings"
for (year in 1:10){
try({
load("amazon/rdas/sequencesexp6.RData")
FLAGS <- flags(
flag_integer("epochs", 2),
flag_integer("batch_size", 128)
)
# Data Preparation --------------------------------------------------------
# Parameters --------------------------------------------------------------
embedding_dims <- 100
filters <- 200
kernel_size <- 10
hidden_dims <- 200
main_input <- layer_input(shape = c(maxlen), dtype = "int32")
ccn_out <- main_input %>%
layer_embedding(vocab_size, embedding_dims, input_length = maxlen) %>%
layer_dropout(0.1) %>%
layer_conv_1d(
filters, kernel_size,
padding = "valid", activation = "relu", strides = 1
) %>%
layer_global_max_pooling_1d() %>%
layer_dense(hidden_dims)
auxiliary_input <- layer_input(shape = c(max_sequence))
entities_out <- auxiliary_input
auxiliary_input_types <- layer_input(shape = c(max_sequence_types))
types_out <- auxiliary_input_types
main_output <- layer_concatenate(c(ccn_out, entities_out, types_out)) %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dropout(0.2) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1, activation = 'sigmoid')
model <- keras_model(
inputs = c(main_input, auxiliary_input, auxiliary_input_types),
outputs = main_output
)
# Compile model
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
history <- model %>%
fit(
x = list(train_vec$new_textParser, sequences, sequences_types),
y = array(dados_train$resposta),
batch_size = FLAGS$batch_size,
epochs = FLAGS$epochs,
validation_split = 0.2
)
history
predictions <- model %>% predict(list(test_vec$new_textParser, sequences_test, sequences_test_types))
predictions2 <- round(predictions, 0)
matriz <- confusionMatrix(data = as.factor(predictions2), as.factor(dados_test$resposta), positive="1")
resultados <- addRowAdpater(resultados, DESC, matriz)
})
}
library(caret)
library(dplyr)
for (year in 1:5) {
svmResults <- readRDS(file = paste0("ensemble/resultados/exp6/svm", year, ".rds"))
# rfResults <- readRDS(file = paste0("ensemble/resultados/exp6/rf", year, ".rds"))
rfResults <- readRDS(file = paste0("ensemble/resultados/exp6/nb", year, ".rds"))
nnResults <- readRDS(file = paste0("ensemble/resultados/exp6/neural", year, ".rds"))
# load("ensemble/datasets/exp1/2-Gram-dbpedia-types-enriquecimento-info-q1-not-null_info_entidades.Rda")
#load("ensemble/datasets/exp2/2-Gram-dbpedia-types-enriquecimento-info-q2-not-null_info_entidades.Rda")
#load("ensemble/datasets/exp3/2-Gram-dbpedia-types-enriquecimento-info-q3-not-null_info_entidades.Rda")
#load("rdas/2gram-entidades-hora-erro-semkeywords.Rda")
# load("chat/rdas/2gram-entidades-erro-sem-key-words.Rda")
load("amazon/rdas/2gram-entidades-erro.Rda")
trainIndex <- readRDS(file = paste0("ensemble/resample/exp6/", "trainIndex", year, ".rds"))
data_train <- as.data.frame(unclass(maFinal[ trainIndex,]))
data_test <- maFinal[-trainIndex,]
bigDataFrame <- bind_cols(list(as.numeric(as.character(svmResults)), as.numeric(as.character(nnResults)), as.numeric(as.character(rfResults))))
bigDataFrameSum <- rowSums(bigDataFrame)
result <- bigDataFrameSum / 3
pred <- round(result,0)
matriz <- confusionMatrix(data = as.factor(pred), as.factor(data_test$resposta), positive="1")
print(matriz$byClass["F1"])
print(matriz$byClass["Precision"])
print(matriz$byClass["Recall"])
}
library(caret)
library(dplyr)
for (year in 1:5) {
svmResults <- readRDS(file = paste0("ensemble/resultados/exp6/svm", year, ".rds"))
rfResults <- readRDS(file = paste0("ensemble/resultados/exp6/rf", year, ".rds"))
# rfResults <- readRDS(file = paste0("ensemble/resultados/exp6/nb", year, ".rds"))
nnResults <- readRDS(file = paste0("ensemble/resultados/exp6/neural", year, ".rds"))
# load("ensemble/datasets/exp1/2-Gram-dbpedia-types-enriquecimento-info-q1-not-null_info_entidades.Rda")
#load("ensemble/datasets/exp2/2-Gram-dbpedia-types-enriquecimento-info-q2-not-null_info_entidades.Rda")
#load("ensemble/datasets/exp3/2-Gram-dbpedia-types-enriquecimento-info-q3-not-null_info_entidades.Rda")
#load("rdas/2gram-entidades-hora-erro-semkeywords.Rda")
# load("chat/rdas/2gram-entidades-erro-sem-key-words.Rda")
load("amazon/rdas/2gram-entidades-erro.Rda")
trainIndex <- readRDS(file = paste0("ensemble/resample/exp6/", "trainIndex", year, ".rds"))
data_train <- as.data.frame(unclass(maFinal[ trainIndex,]))
data_test <- maFinal[-trainIndex,]
bigDataFrame <- bind_cols(list(as.numeric(as.character(svmResults)), as.numeric(as.character(nnResults)), as.numeric(as.character(rfResults))))
bigDataFrameSum <- rowSums(bigDataFrame)
result <- bigDataFrameSum / 3
pred <- round(result,0)
matriz <- confusionMatrix(data = as.factor(pred), as.factor(data_test$resposta), positive="1")
print(matriz$byClass["F1"])
print(matriz$byClass["Precision"])
print(matriz$byClass["Recall"])
}
epochs <- c(2,3,4)
for (i in epochs) {
print(epochs)
}
epochs <- c(2,3,4)
for (i in epochs) {
print(i)
}
for (epoch in epochs) {
print(i)
}
epochs <- c(2,3,4)
for (epoch in epochs) {
print(epoch)
}
epochs <- c(2,3,4)
batchs <- c(32, 64, 128, 164, 200)
for (epoch in epochs) {
print(epoch)
for (batch in batchs) {
print(batch)
}
}
