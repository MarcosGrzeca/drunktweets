library(keras)
library(readr)
library(stringr)
library(purrr)
library(tibble)
library(dplyr)
library(tools)
source(file_path_as_absolute("utils/getDados.R"))
library(keras)
library(readr)
library(stringr)
library(purrr)
library(tibble)
library(dplyr)
library(tools)
source(file_path_as_absolute("utils/getDados.R"))
max_features <- 5000
dados <- getDados()
source(file_path_as_absolute("utils/getDados.R"))
source(file_path_as_absolute("utils/getDados.R"))
dados <- getDados()
source(file_path_as_absolute("utils/getDados.R"))
dados <- getDados()
View(dados)
View(dados)
set.seed(10)
split=0.80
trainIndex <- createDataPartition(dados$resposta, p=split, list=FALSE)
data_train <- as.data.frame(unclass(dados[ trainIndex,]))
data_test <- dados[-trainIndex,]
y_train <- dados$resposta[training_indices]
View(x_train)
x_train <- as.data.frame(unclass(dados[ trainIndex,]))
View(x_train)
y_train <- x_train$resposta
dados <- getDados()
set.seed(10)
split=0.80
trainIndex <- createDataPartition(dados$resposta, p=split, list=FALSE)
dados_train <- as.data.frame(unclass(dados[ trainIndex,]))
x_train <- dados_train$textOriginal
y_train <- dados_train$resposta
dados_test <- dados[-trainIndex,]
x_test <- dados_test$textOriginal
y_test <- dados_test$resposta
y_test
dados <- processarDados(x_train, maxlen, max_features)
maxlen <- 39
dados <- processarDados(x_train, maxlen, max_features)
summary(dadosProcessados)
dadosProcessados <- processarDados(x_train, maxlen, max_features)
summary(dadosProcessados)
View(dadosProcessados)
model <- keras_model_sequential()
model %>%
# Start off with an efficient embedding layer which maps
# the vocab indices into embedding_dims dimensions
layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%
layer_dropout(0.2) %>%
# Add a Convolution1D, which will learn filters
# Word group filters of size filter_length:
layer_conv_1d(
filters, kernel_size,
padding = "valid", activation = "relu", strides = 1
) %>%
# Apply max pooling:
layer_global_max_pooling_1d() %>%
# Add a vanilla hidden layer:
layer_dense(hidden_dims) %>%
# Apply 20% layer dropout
layer_dropout(0.2) %>%
layer_activation("relu") %>%
# Project onto a single unit output layer, and squash it with a sigmoid
layer_dense(1) %>%
layer_activation("sigmoid")
batch_size <- 16
epochs <- 3
embedding_dims <- 50
filters <- 250
kernel_size <- 3
hidden_dims <- 250
model %>%
# Start off with an efficient embedding layer which maps
# the vocab indices into embedding_dims dimensions
layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%
layer_dropout(0.2) %>%
# Add a Convolution1D, which will learn filters
# Word group filters of size filter_length:
layer_conv_1d(
filters, kernel_size,
padding = "valid", activation = "relu", strides = 1
) %>%
# Apply max pooling:
layer_global_max_pooling_1d() %>%
# Add a vanilla hidden layer:
layer_dense(hidden_dims) %>%
# Apply 20% layer dropout
layer_dropout(0.2) %>%
layer_activation("relu") %>%
# Project onto a single unit output layer, and squash it with a sigmoid
layer_dense(1) %>%
layer_activation("sigmoid")
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
history <- model %>%
fit(
x_train, y_train,
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2
)
history <- model %>%
fit(
x_train, labels_train,
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2
)
x_train
history <- model %>%
fit(
dadosProcessados, labels_train,
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2
)
labels_train <- as.array(as.numeric(y_train))
history <- model %>%
fit(
dadosProcessados, labels_train,
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2
)
library(keras)
library(readr)
library(purrr)
library(tibble)
library(dplyr)
library(stringr)
library(tools)
source(file_path_as_absolute("utils/getDados.R"))
dados <- getDados()
View(dados)
library(keras)
library(readr)
library(stringr)
library(purrr)
library(tibble)
library(dplyr)
library(tools)
source(file_path_as_absolute("utils/getDados.R"))
dados <- getDados()
View(dados)
#Separação teste e treinamento
set.seed(10)
split=0.80
trainIndex <- createDataPartition(dados$resposta, p=split, list=FALSE)
dados_train <- as.data.frame(dados[ trainIndex,])
View(dados_train)
x_train <- dados_train$textOriginal
y_train <- dados_train$resposta
labels_train <- as.array(as.numeric(y_train))
trainIndex <- createDataPartition(dados$resposta, p=split, list=FALSE)
dados_train <- dados[ trainIndex,]
View(dados_train)
x_train <- dados_train$textOriginal
y_train <- dados_train$resposta
labels_train <- as.array(as.numeric(y_train))
dados_test <- dados[-trainIndex,]
x_test <- dados_test$textOriginal
y_test <- dados_test$resposta
labels_test <- as.array(as.numeric(y_test))
x_train
str(x_train)
nrow(x_train)
nrow(dados)
dados_train <- as.data.frame(dados[ trainIndex,])
x_train <- dados_train$textOriginal
y_train <- dados_train$resposta
str(x_train)
nrow(x_train)
nrow(dados)
x_train <- subset(dados_train, select = -c(textOriginal))
y_train <- subset(dados_train, select = -c(resposta))
labels_train <- as.array(as.numeric(y_train))
x_train <- subset(dados_train, select = c(textOriginal))
y_train <- subset(dados_train, select = c(resposta))
labels_train <- as.array(as.numeric(y_train))
str(x_train)
nrow(x_train)
nrow(dados)
dados_test <- dados[-trainIndex,]
x_test <- subset(dados_test, select = c(textOriginal))
y_test <- subset(dados_test, select = c(textOriginal))
labels_test <- as.array(as.numeric(y_test))
str(x_train)
labels_test <- as.array(as.numeric(y_test))
y_test <- subset(dados_test, select = c(resposta))
dados_test <- dados[-trainIndex,]
x_test <- subset(dados_test, select = c(textOriginal))
y_test <- subset(dados_test, select = c(resposta))
labels_test <- as.array(as.numeric(y_test))
y_test
labels_test <- as.array(as.numeric(y_test))
labels_train <- as.array(as.numeric(y_train))
y_train <- subset(dados_train, select = c(resposta))
library(keras)
library(readr)
library(stringr)
library(purrr)
library(tibble)
library(dplyr)
library(tools)
source(file_path_as_absolute("utils/getDados.R"))
dados <- getDados()
View(dados)
set.seed(10)
split=0.80
trainIndex <- createDataPartition(dados$resposta, p=split, list=FALSE)
dados_train <- as.data.frame(dados[ trainIndex,])
x_train <- subset(dados_train, select = c(textOriginal))
y_train <- subset(dados_train, select = c(resposta))
labels_train <- as.array(as.numeric(y_train))
y_train
y_train <- dados_train$resposta
y_train
labels_train <- as.array(as.numeric(y_train))
dados_test <- dados[-trainIndex,]
dados_test <- dados[-trainIndex,]
x_test <- subset(dados_test, select = c(textOriginal))
y_test <- dados_test$resposta
labels_test <- as.array(as.numeric(y_test))
max_features <- 5000
maxlen <- 50
batch_size <- 16
epochs <- 3
embedding_dims <- 50
filters <- 250
kernel_size <- 3
hidden_dims <- 250
dadosProcessados <- processarDados(x_train, maxlen, max_features)
model <- keras_model_sequential()
max_features <- 10000
dadosProcessados <- processarDados(x_train, maxlen, max_features)
str(dadosProcessados)
model <- keras_model_sequential()
model %>%
# Start off with an efficient embedding layer which maps
# the vocab indices into embedding_dims dimensions
layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%
layer_dropout(0.2) %>%
# Add a Convolution1D, which will learn filters
# Word group filters of size filter_length:
layer_conv_1d(
filters, kernel_size,
padding = "valid", activation = "relu", strides = 1
) %>%
# Apply max pooling:
layer_global_max_pooling_1d() %>%
# Add a vanilla hidden layer:
layer_dense(hidden_dims) %>%
# Apply 20% layer dropout
layer_dropout(0.2) %>%
layer_activation("relu") %>%
# Project onto a single unit output layer, and squash it with a sigmoid
layer_dense(1) %>%
layer_activation("sigmoid")
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
history <- model %>%
fit(
dadosProcessados, labels_train,
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2
)
x_train
View(x_train)
View(dadosProcessados)
library(keras)
library(readr)
library(stringr)
library(purrr)
library(tibble)
library(dplyr)
library(tools)
source(file_path_as_absolute("utils/getDados.R"))
dados <- getDados()
View(dados)
#Separação teste e treinamento
set.seed(10)
split=0.80
trainIndex <- createDataPartition(dados$resposta, p=split, list=FALSE)
#dados_train <- as.data.frame(unclass(dados[ trainIndex,]))
dados_train <- as.data.frame(dados[ trainIndex,])
x_train <- subset(dados_train, select = c(textOriginal))
y_train <- dados_train$resposta
labels_train <- as.array(as.numeric(y_train))
View(x_train)
dados_test <- dados[-trainIndex,]
x_test <- subset(dados_test, select = c(textOriginal))
y_test <- dados_test$resposta
labels_test <- as.array(as.numeric(y_test))
#VERIFICAR
# Data Preparation --------------------------------------------------------s
max_features <- 10000
maxlen <- 50
# Parameters --------------------------------------------------------------
batch_size <- 32
epochs <- 3
embedding_dims <- 100
filters <- 250
kernel_size <- 3
hidden_dims <- 250
## PRÉ-PROCESSAMENTO
dadosProcessados <- processarDados(x_train, maxlen, max_features)
View(dadosProcessados)
model <- keras_model_sequential()
model %>%
# Start off with an efficient embedding layer which maps
# the vocab indices into embedding_dims dimensions
layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%
layer_dropout(0.2) %>%
# Add a Convolution1D, which will learn filters
# Word group filters of size filter_length:
layer_conv_1d(
filters, kernel_size,
padding = "valid", activation = "relu", strides = 1
) %>%
# Apply max pooling:
layer_global_max_pooling_1d() %>%
# Add a vanilla hidden layer:
layer_dense(hidden_dims) %>%
# Apply 20% layer dropout
layer_dropout(0.2) %>%
layer_activation("relu") %>%
# Project onto a single unit output layer, and squash it with a sigmoid
layer_dense(1) %>%
layer_activation("sigmoid")
# Compile model
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
# Training ----------------------------------------------------------------
history <- model %>%
fit(
dadosProcessados, labels_train,
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2
)
library(keras)
library(readr)
library(stringr)
library(purrr)
library(tibble)
library(dplyr)
library(tools)
source(file_path_as_absolute("utils/getDados.R"))
dados <- getDados()
View(dados)
#Separação teste e treinamento
set.seed(10)
split=0.80
trainIndex <- createDataPartition(dados$resposta, p=split, list=FALSE)
#dados_train <- as.data.frame(unclass(dados[ trainIndex,]))
dados_train <- as.data.frame(dados[ trainIndex,])
x_train <- subset(dados_train, select = c(textOriginal))
y_train <- dados_train$resposta
labels_train <- as.array(as.numeric(y_train))
View(x_train)
dados_test <- dados[-trainIndex,]
x_test <- subset(dados_test, select = c(textOriginal))
y_test <- dados_test$resposta
labels_test <- as.array(as.numeric(y_test))
#VERIFICAR
# Data Preparation --------------------------------------------------------s
max_features <- 10000
maxlen <- 50
# Parameters --------------------------------------------------------------
batch_size <- 32
epochs <- 3
embedding_dims <- 100
filters <- 250
kernel_size <- 3
hidden_dims <- 250
## PRÉ-PROCESSAMENTO
dadosProcessados <- processarDados(dados_train$textOriginal, maxlen, max_features)
View(dadosProcessados)
model <- keras_model_sequential()
model %>%
# Start off with an efficient embedding layer which maps
# the vocab indices into embedding_dims dimensions
layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%
layer_dropout(0.2) %>%
# Add a Convolution1D, which will learn filters
# Word group filters of size filter_length:
layer_conv_1d(
filters, kernel_size,
padding = "valid", activation = "relu", strides = 1
) %>%
# Apply max pooling:
layer_global_max_pooling_1d() %>%
# Add a vanilla hidden layer:
layer_dense(hidden_dims) %>%
# Apply 20% layer dropout
layer_dropout(0.2) %>%
layer_activation("relu") %>%
# Project onto a single unit output layer, and squash it with a sigmoid
layer_dense(1) %>%
layer_activation("sigmoid")
# Compile model
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
# Training ----------------------------------------------------------------
history <- model %>%
fit(
dadosProcessados, dados_train$resposta,
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2
)
dadosProcessados
dados_train$resposta
history <- model %>%
fit(
dadosProcessados, dados_train$resposta,
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2
)
history <- model %>%
fit(
dadosProcessados, y,
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2
)
history <- model %>%
fit(
dados_train$resposta, dados_train$resposta,
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2
)
history <- model %>%
fit(
dados_train$resposta, dados_train$resposta,
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2
)
history <- model %>%
fit(
dadosProcessados, as.array(as.numeric(dados_train$resposta),
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2
)
)
history <- model %>%
fit(
dadosProcessados, as.array(as.numeric(dados_train$resposta)),
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2
)
history <- model %>%
fit(
dadosProcessados, as.array(dados_train$resposta),
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2
)
history <- model %>%
fit(
dadosProcessados, as.numeric(dados_train$resposta),
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2
)
as.numeric(dados_train$resposta)
str(dados_train$resposta)
summary(dados_train$resposta)
history <- model %>%
fit(
dadosProcessados, dados_train$resposta,
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2
)
history <- model %>%
fit(
dadosProcessados, array(dados_train$resposta),
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2
)
