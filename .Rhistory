matriz <- confusionMatrix(data = as.factor(pred), as.factor(dados_test$resposta), positive="1")
# out-of-sample accuracy
pred <- predict(classifier, subset(dados_test, select = -c(resposta)))
pred
# bigDataFrame <- as.numeric(as.character(svmResults))
matriz <- confusionMatrix(data = as.factor(pred), as.factor(dados_test$resposta), positive="1")
print(matriz$byClass["F1"])
print(matriz$byClass["Precision"])
print(matriz$byClass["Recall"])
resultados <- addRowAdpaterBamos(resultados, matriz$byClass["F1"], matriz$byClass["Precision"], matriz$byClass["Recall"])
library(caret)
library(dplyr)
library(doMC)
library(mlbench)
library(magrittr)
CORES <- 5
registerDoMC(CORES)
expName <- "exp3"
resultados <- data.frame(matrix(ncol = 4, nrow = 0))
names(resultados) <- c("F1", "Precision", "Recall")
addRowAdpaterBamos <- function(resultados, f1, precision, recall) {
newRes <- data.frame(f1, precision, recall)
rownames(newRes) <- "Exp"
names(newRes) <- c("F1", "Precision", "Recall")
newdf <- rbind(resultados, newRes)
return (newdf)
}
treinarPoly <- function(data_train, resposta) {
fit <- train(
x = data_train,
y = resposta,
method = "svmPoly",
trControl = trainControl(method = "cv", number = 5, savePred=T))
return (fit)
}
for (year in 1:5) {
svmResults <- readRDS(file = paste0("ensembles/ensemblev3/resultados/", expName, "/svm", year, ".rds"))
xgboost <- readRDS(file = paste0("ensembles/ensemblev4/resultados/", expName, "/xgboost", year, ".rds"))
nnResults <- readRDS(file = paste0("ensembles/ensemblev3/resultados/", expName, "/newdl/neuralprob", year, ".rds"))
if (expName == "exp1") {
datasetFile <-"ensembles/ensemble/datasets/exp1/2-Gram-dbpedia-types-enriquecimento-info-q1-not-null_info_entidades.Rda"
} else if (expName == "exp2") {
datasetFile <-"ensembles/ensemble/datasets/exp2/2-Gram-dbpedia-types-enriquecimento-info-q2-not-null_info_entidades.Rda"
} else if (expName == "exp3") {
datasetFile <-"ensembles/ensemble/datasets/exp3/2-Gram-dbpedia-types-enriquecimento-info-q3-not-null_info_entidades.Rda"
} else if (expName == "ds2") {
datasetFile <-"chat/rdas/2gram-entidades-erro-sem-key-words_orderbyid.Rda"
} else if (expName == "ds3") {
datasetFile <-"amazon/rdas/2gram-entidades-erro.Rda"
}
load(datasetFile)
trainIndex <- readRDS(file = paste0("ensembles/ensemblev2/resample/", expName, "/", "trainIndex", year, ".rds"))
data_test <- maFinal[-trainIndex,]
bigDataFrame <- bind_cols(list(as.numeric(as.character(svmResults)), as.numeric(as.character(xgboost)), as.numeric(as.character(nnResults))))
library(caret)
trainIndex <- createDataPartition(data_test$resposta, p=0.9, list=FALSE)
dados_train <- data_test[ trainIndex,]
dados_test <- data_test[-trainIndex,]
classifier <- treinarPoly(subset(dados_train, select = -c(resposta)), dados_train$resposta)
# out-of-sample accuracy
pred <- predict(classifier, subset(dados_test, select = -c(resposta)))
pred
#resultados <- addRowSimple(resultados, "Com", round(precision(preds>.50, resposta_test) * 100,6), round(recall(preds>.50, resposta_test) * 100,6))
#bigDataFrame <- bind_cols(list(as.numeric(as.character(svmResults)), as.numeric(as.character(xgboost))))
#pred <- round(rowMeans(bigDataFrame),0)
# bigDataFrame <- as.numeric(as.character(svmResults))
matriz <- confusionMatrix(data = as.factor(pred), as.factor(dados_test$resposta), positive="1")
print(matriz$byClass["F1"])
print(matriz$byClass["Precision"])
print(matriz$byClass["Recall"])
resultados <- addRowAdpaterBamos(resultados, matriz$byClass["F1"], matriz$byClass["Precision"], matriz$byClass["Recall"])
}
View(resultados)
mean(resultados$F1 * 100)
mean(resultados$Precision * 100)
mean(resultados$Recall * 100)
resultados
library(tools)
library(tm)
source(file_path_as_absolute("ipm/experimenters.R"))
source(file_path_as_absolute("utils/getDados.R"))
source(file_path_as_absolute("baseline/dados.R"))
source(file_path_as_absolute("utils/tokenizer.R"))
dados <- getDadosBaselineByQ("q1")
# dados$textEmbedding <- removePunctuation(dados$textEmbedding)
maxlen <- 38
max_words <- 7860
tokenizer <-  text_tokenizer(num_words = max_words) %>%
fit_text_tokenizer(dados$textEmbedding)
sequences <- texts_to_sequences(tokenizer, dados$textEmbedding)
word_index = tokenizer$word_index
vocab_size <- length(word_index)
vocab_size <- vocab_size + 1
vocab_size
cat("Found", length(word_index), "unique tokens.\n")
data <- pad_sequences(sequences, maxlen = maxlen)
library(caret)
trainIndex <- createDataPartition(dados$resposta, p=0.8, list=FALSE)
dados_train <- dados[ trainIndex,]
dados_test <- dados[-trainIndex,]
dados_train_sequence <- data[ trainIndex,]
dados_test_sequence <- data[-trainIndex,]
max_words <- vocab_size
word_index <- tokenizer$word_index
# Data Preparation --------------------------------------------------------
# Parameters --------------------------------------------------------------
embedding_dims <- 100
# Parameters --------------------------------------------------------------
# filters <- 200
filters <- 164
main_input <- layer_input(shape = c(maxlen), dtype = "int32")
main_output <- main_input %>%
layer_embedding(vocab_size, embedding_dims, input_length = maxlen, name = "embedding") %>%
bidirectional(
layer_lstm(units = 128, return_sequences = TRUE) %>%
layer_lstm(units = 64, return_sequences = TRUE, recurrent_dropout = 0.2) %>%
layer_lstm(units = 32) %>%
) %>%
layer_dense(units = 1, activation = 'sigmoid')
model <- keras_model(
inputs = c(main_input),
outputs = main_output
)
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
library(keras)
# Training ----------------------------------------------------------------
history <- model %>%
fit(
x = list(dados_train_sequence),
y = array(dados_train$resposta),
batch_size = 64,
epochs = 10,
#callbacks = callbacks_list,
validation_split = 0.2
)
addRowAdpater <- function(resultados, baseline, matriz, ...) {
newRes <- data.frame(baseline, matriz$byClass["F1"] * 100, matriz$byClass["Precision"] * 100, matriz$byClass["Recall"] * 100)
rownames(newRes) <- baseline
names(newRes) <- c("Baseline", "F1", "Precision", "Recall")
newdf <- rbind(resultados, newRes)
return (newdf)
}
resultados <- data.frame(matrix(ncol = 4, nrow = 0))
names(resultados) <- c("Baseline", "F1", "Precisão", "Revocação")
predictions <- model %>% predict(list(dados_test_sequence))
predictions2 <- round(predictions, 0)
matriz <- confusionMatrix(data = as.factor(predictions2), as.factor(dados_test$resposta), positive="1")
resultados <- addRowAdpater(resultados, "TESTE", matriz)
##
library(dplyr)
embedding_matrixTwo <- get_weights(model)[[1]]
words <- data_frame(
word = names(tokenizer$word_index),
id = as.integer(unlist(tokenizer$word_index))
)
words <- words %>%
filter(id <= tokenizer$num_words) %>%
arrange(id)
row.names(embedding_matrixTwo) <- c("UNK", words$word)
embedding_file <- "ipmbilstm/exportembedding/ds1/q1/bilstm_10_epocas.txt"
write.table(embedding_matrixTwo, embedding_file, sep=" ",row.names=TRUE)
system(paste0("sed -i 's/\"//g' ", embedding_file))
install.packages("tm")
install.packages("RMySQL")
library(tools)
library(tm)
source(file_path_as_absolute("ipm/experimenters.R"))
source(file_path_as_absolute("utils/getDados.R"))
install.packages("rio")
library(tools)
library(tm)
source(file_path_as_absolute("ipm/experimenters.R"))
source(file_path_as_absolute("utils/getDados.R"))
source(file_path_as_absolute("baseline/dados.R"))
source(file_path_as_absolute("utils/tokenizer.R"))
dados <- getDadosBaselineByQ("q1")
# dados$textEmbedding <- removePunctuation(dados$textEmbedding)
View(dados)
maxlen <- 38
max_words <- 7860
tokenizer <-  text_tokenizer(num_words = max_words) %>%
fit_text_tokenizer(dados$textEmbedding)
sequences <- texts_to_sequences(tokenizer, dados$textEmbedding)
word_index = tokenizer$word_index
vocab_size <- length(word_index)
vocab_size <- vocab_size + 1
vocab_size
cat("Found", length(word_index), "unique tokens.\n")
data <- pad_sequences(sequences, maxlen = maxlen)
library(caret)
trainIndex <- createDataPartition(dados$resposta, p=0.8, list=FALSE)
dados_train <- dados[ trainIndex,]
dados_test <- dados[-trainIndex,]
dados_train_sequence <- data[ trainIndex,]
dados_test_sequence <- data[-trainIndex,]
max_words <- vocab_size
word_index <- tokenizer$word_index
# Data Preparation --------------------------------------------------------
# Parameters --------------------------------------------------------------
embedding_dims <- 100
# Parameters --------------------------------------------------------------
# filters <- 200
filters <- 164
main_input <- layer_input(shape = c(maxlen), dtype = "int32")
main_output <- main_input %>%
layer_embedding(vocab_size, embedding_dims, input_length = maxlen, name = "embedding") %>%
bidirectional(
layer_lstm(units = 128, return_sequences = TRUE) %>%
layer_lstm(units = 64, return_sequences = TRUE, recurrent_dropout = 0.2) %>%
layer_lstm(units = 32) %>%
) %>%
layer_dense(units = 1, activation = 'sigmoid')
main_input <- layer_input(shape = c(maxlen), dtype = "int32")
main_output <- main_input %>%
layer_embedding(vocab_size, embedding_dims, input_length = maxlen, name = "embedding") %>%
bidirectional(
layer_lstm(units = 128, return_sequences = TRUE) %>%
layer_lstm(units = 64, return_sequences = TRUE, recurrent_dropout = 0.2) %>%
layer_lstm(units = 32)
) %>%
layer_dense(units = 1, activation = 'sigmoid')
main_input <- layer_input(shape = c(maxlen), dtype = "int32")
main_output <- main_input %>%
layer_embedding(vocab_size, embedding_dims, input_length = maxlen, name = "embedding") %>%
bidirectional(
layer_lstm(units = 128, return_sequences = TRUE) %>%
layer_lstm(units = 64, return_sequences = TRUE, recurrent_dropout = 0.2) %>%
layer_lstm(units = 32)
) %>%
layer_dense(units = 1, activation = 'sigmoid')
main_output <- main_input %>%
layer_embedding(vocab_size, embedding_dims, input_length = maxlen, name = "embedding") %>%
bidirectional(
layer_lstm(units = 32)
) %>%
layer_dense(units = 1, activation = 'sigmoid')
main_output <- main_input %>%
layer_embedding(vocab_size, embedding_dims, input_length = maxlen, name = "embedding") %>%
bidirectional(
layer_lstm(units = 128, return_sequences = TRUE) %>%
layer_lstm(units = 64, return_sequences = TRUE, recurrent_dropout = 0.2) %>%
layer_lstm(units = 32)
) %>%
layer_dense(units = 1, activation = 'sigmoid')
main_input <- layer_input(shape = c(maxlen), dtype = "int32")
main_output <- main_input %>%
layer_embedding(vocab_size, embedding_dims, input_length = maxlen, name = "embedding") %>%
bidirectional(
layer_lstm(units = 128, return_sequences = TRUE)
) %>%
bidirectional(
layer_lstm(units = 64, return_sequences = TRUE, recurrent_dropout = 0.2)
) %>%
bidirectional(
layer_lstm(units = 32)
) %>%
layer_dense(units = 1, activation = 'sigmoid')
model <- keras_model(
inputs = c(main_input),
outputs = main_output
)
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
library(keras)
# Training ----------------------------------------------------------------
history <- model %>%
fit(
x = list(dados_train_sequence),
y = array(dados_train$resposta),
batch_size = 64,
epochs = 10,
#callbacks = callbacks_list,
validation_split = 0.2
)
addRowAdpater <- function(resultados, baseline, matriz, ...) {
newRes <- data.frame(baseline, matriz$byClass["F1"] * 100, matriz$byClass["Precision"] * 100, matriz$byClass["Recall"] * 100)
rownames(newRes) <- baseline
names(newRes) <- c("Baseline", "F1", "Precision", "Recall")
newdf <- rbind(resultados, newRes)
return (newdf)
}
resultados <- data.frame(matrix(ncol = 4, nrow = 0))
names(resultados) <- c("Baseline", "F1", "Precisão", "Revocação")
predictions <- model %>% predict(list(dados_test_sequence))
predictions2 <- round(predictions, 0)
matriz <- confusionMatrix(data = as.factor(predictions2), as.factor(dados_test$resposta), positive="1")
resultados <- addRowAdpater(resultados, "TESTE", matriz)
View(resultados)
##
library(dplyr)
embedding_matrixTwo <- get_weights(model)[[1]]
words <- data_frame(
word = names(tokenizer$word_index),
id = as.integer(unlist(tokenizer$word_index))
)
words <- words %>%
filter(id <= tokenizer$num_words) %>%
arrange(id)
row.names(embedding_matrixTwo) <- c("UNK", words$word)
embedding_file <- "ipmbilstm/exportembedding/ds1/q1/bilstm_10_epocas.txt"
write.table(embedding_matrixTwo, embedding_file, sep=" ",row.names=TRUE)
embedding_file <- "ipmbilstm/exportembedding/ds1/q1/bilstm_10_epocas.txt"
write.table(embedding_matrixTwo, embedding_file, sep=" ",row.names=TRUE)
embedding_file <- "ipmbilstm/exportembedding/ds1/q1/bilstm_10_epocas.txt"
write.table(embedding_matrixTwo, embedding_file, sep=" ",row.names=TRUE)
system(paste0("sed -i 's/\"//g' ", embedding_file))
install.packages(c("lsa", "quanteda"))
library(tools)
library(rword2vec)
install.packages("rword2vec")
library(tools)
library(rword2vec)
library(tools)
source(file_path_as_absolute("ipm/experimenters.R"))
source(file_path_as_absolute("utils/getDados.R"))
source(file_path_as_absolute("baseline/dados.R"))
source(file_path_as_absolute("utils/tokenizer.R"))
embeddingsFile <- "ipmbilstm/exportembedding/ds1/q1_representacao_bilstm_pca.RData"
try({
source(file_path_as_absolute("exp4/svmpoly/svmvalidator.R"))
})
library(tools)
library(tm)
source(file_path_as_absolute("ipm/experimenters.R"))
source(file_path_as_absolute("utils/getDados.R"))
source(file_path_as_absolute("baseline/dados.R"))
source(file_path_as_absolute("utils/tokenizer.R"))
dados <- getDadosBaselineByQ("q1")
# dados$textEmbedding <- removePunctuation(dados$textEmbedding)
View(dados)
maxlen <- 38
max_words <- 7860
tokenizer <-  text_tokenizer(num_words = max_words) %>%
fit_text_tokenizer(dados$textEmbedding)
sequences <- texts_to_sequences(tokenizer, dados$textEmbedding)
word_index = tokenizer$word_index
vocab_size <- length(word_index)
vocab_size <- vocab_size + 1
vocab_size
cat("Found", length(word_index), "unique tokens.\n")
data <- pad_sequences(sequences, maxlen = maxlen)
library(caret)
trainIndex <- createDataPartition(dados$resposta, p=0.8, list=FALSE)
dados_train <- dados[ trainIndex,]
dados_test <- dados[-trainIndex,]
dados_train_sequence <- data[ trainIndex,]
dados_test_sequence <- data[-trainIndex,]
max_words <- vocab_size
word_index <- tokenizer$word_index
# Data Preparation --------------------------------------------------------
# Parameters --------------------------------------------------------------
embedding_dims <- 100
# Parameters --------------------------------------------------------------
# filters <- 200
filters <- 164
main_input <- layer_input(shape = c(maxlen), dtype = "int32")
main_output <- main_input %>%
layer_embedding(vocab_size, embedding_dims, input_length = maxlen, name = "embedding") %>%
bidirectional(
layer_lstm(units = 128, return_sequences = TRUE)
) %>%
bidirectional(
layer_lstm(units = 64, return_sequences = TRUE, recurrent_dropout = 0.2)
) %>%
bidirectional(
layer_lstm(units = 32)
) %>%
layer_dense(units = 1, activation = 'sigmoid')
model <- keras_model(
inputs = c(main_input),
outputs = main_output
)
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
library(keras)
# Training ----------------------------------------------------------------
history <- model %>%
fit(
x = list(dados_train_sequence),
y = array(dados_train$resposta),
batch_size = 64,
epochs = 10,
#callbacks = callbacks_list,
validation_split = 0.2
)
addRowAdpater <- function(resultados, baseline, matriz, ...) {
newRes <- data.frame(baseline, matriz$byClass["F1"] * 100, matriz$byClass["Precision"] * 100, matriz$byClass["Recall"] * 100)
rownames(newRes) <- baseline
names(newRes) <- c("Baseline", "F1", "Precision", "Recall")
newdf <- rbind(resultados, newRes)
return (newdf)
}
resultados <- data.frame(matrix(ncol = 4, nrow = 0))
names(resultados) <- c("Baseline", "F1", "Precisão", "Revocação")
predictions <- model %>% predict(list(dados_test_sequence))
predictions2 <- round(predictions, 0)
matriz <- confusionMatrix(data = as.factor(predictions2), as.factor(dados_test$resposta), positive="1")
resultados <- addRowAdpater(resultados, "TESTE", matriz)
##
library(dplyr)
embedding_matrixTwo <- get_weights(model)[[1]]
words <- data_frame(
word = names(tokenizer$word_index),
id = as.integer(unlist(tokenizer$word_index))
)
words <- words %>%
filter(id <= tokenizer$num_words) %>%
arrange(id)
row.names(embedding_matrixTwo) <- c("UNK", words$word)
embedding_file <- "ipmbilstm/exportembedding/ds1/q1/bilstm_10_epocas_fake.txt"
write.table(embedding_matrixTwo, embedding_file, sep=" ",row.names=TRUE)
system(paste0("sed -i 's/\"//g' ", embedding_file))
resultados
library(tools)
library(tm)
source(file_path_as_absolute("ipm/experimenters.R"))
source(file_path_as_absolute("utils/getDados.R"))
source(file_path_as_absolute("baseline/dados.R"))
source(file_path_as_absolute("utils/tokenizer.R"))
dados <- getDadosBaselineByQ("q1")
# dados$textEmbedding <- removePunctuation(dados$textEmbedding)
View(dados)
maxlen <- 38
max_words <- 7860
tokenizer <-  text_tokenizer(num_words = max_words) %>%
fit_text_tokenizer(dados$textEmbedding)
sequences <- texts_to_sequences(tokenizer, dados$textEmbedding)
word_index = tokenizer$word_index
vocab_size <- length(word_index)
vocab_size <- vocab_size + 1
vocab_size
cat("Found", length(word_index), "unique tokens.\n")
data <- pad_sequences(sequences, maxlen = maxlen)
library(caret)
trainIndex <- createDataPartition(dados$resposta, p=0.8, list=FALSE)
dados_train <- dados[ trainIndex,]
dados_test <- dados[-trainIndex,]
dados_train_sequence <- data[ trainIndex,]
dados_test_sequence <- data[-trainIndex,]
max_words <- vocab_size
word_index <- tokenizer$word_index
# Data Preparation --------------------------------------------------------
# Parameters --------------------------------------------------------------
embedding_dims <- 100
# Parameters --------------------------------------------------------------
# filters <- 200
filters <- 164
main_input <- layer_input(shape = c(maxlen), dtype = "int32")
main_output <- main_input %>%
layer_embedding(vocab_size, embedding_dims, input_length = maxlen, name = "embedding") %>%
layer_lstm(units = 128, return_sequences = TRUE) %>%
layer_lstm(units = 64, return_sequences = TRUE, recurrent_dropout = 0.2) %>%
layer_lstm(units = 32) %>%
layer_dense(units = 1, activation = 'sigmoid')
model <- keras_model(
inputs = c(main_input),
outputs = main_output
)
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
library(keras)
# Training ----------------------------------------------------------------
history <- model %>%
fit(
x = list(dados_train_sequence),
y = array(dados_train$resposta),
batch_size = 64,
epochs = 10,
#callbacks = callbacks_list,
validation_split = 0.2
)
addRowAdpater <- function(resultados, baseline, matriz, ...) {
newRes <- data.frame(baseline, matriz$byClass["F1"] * 100, matriz$byClass["Precision"] * 100, matriz$byClass["Recall"] * 100)
rownames(newRes) <- baseline
names(newRes) <- c("Baseline", "F1", "Precision", "Recall")
newdf <- rbind(resultados, newRes)
return (newdf)
}
resultados <- data.frame(matrix(ncol = 4, nrow = 0))
names(resultados) <- c("Baseline", "F1", "Precisão", "Revocação")
predictions <- model %>% predict(list(dados_test_sequence))
predictions2 <- round(predictions, 0)
matriz <- confusionMatrix(data = as.factor(predictions2), as.factor(dados_test$resposta), positive="1")
resultados <- addRowAdpater(resultados, "TESTE", matriz)
##
library(dplyr)
embedding_matrixTwo <- get_weights(model)[[1]]
words <- data_frame(
word = names(tokenizer$word_index),
id = as.integer(unlist(tokenizer$word_index))
)
words <- words %>%
filter(id <= tokenizer$num_words) %>%
arrange(id)
row.names(embedding_matrixTwo) <- c("UNK", words$word)
embedding_file <- "ipmbilstm/exportembedding/ds1/q1/bilstm_10_epocas_fake.txt"
write.table(embedding_matrixTwo, embedding_file, sep=" ",row.names=TRUE)
system(paste0("sed -i 's/\"//g' ", embedding_file))
resultados
